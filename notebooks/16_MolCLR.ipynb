{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f071009-39e9-4e7f-9dc3-cd1d4cb51f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "613e167d-7cab-43fa-95e0-33dca92f2102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "\n",
    "import openpyxl\n",
    "import os\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fc90c9c-65a9-4e89-ae75-ac0b25a1542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP = Path.cwd().as_posix().replace('notebooks','')\n",
    "raw_dir = Path(TOP) / 'data'/'raw'\n",
    "interim_dir = Path(TOP) / 'data'/'interim'\n",
    "external_dir = Path(TOP) / 'data'/'external'\n",
    "figures_dir = Path(TOP) / 'reports'/'figures/'\n",
    "processed_dir = Path(TOP) / 'data'/'processed'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "572086c2-74e9-42d4-8e4c-f6623f07bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93df9acd-f9da-46ae-9ccd-def64c9a0222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch_scatter import scatter\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdchem import HybridizationType\n",
    "from rdkit.Chem.rdchem import BondType as BT\n",
    "from rdkit.Chem import AllChem\n",
    "import math\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6be51146-6369-4c87-b3dd-07db3825dfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATOM_LIST = list(range(1,119))\n",
    "CHIRALITY_LIST = [\n",
    "    Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "    Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "    Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "    Chem.rdchem.ChiralType.CHI_OTHER\n",
    "]\n",
    "BOND_LIST = [\n",
    "    BT.SINGLE, \n",
    "    BT.DOUBLE, \n",
    "    BT.TRIPLE, \n",
    "    BT.AROMATIC\n",
    "]\n",
    "BONDDIR_LIST = [\n",
    "    Chem.rdchem.BondDir.NONE,\n",
    "    Chem.rdchem.BondDir.ENDUPRIGHT,\n",
    "    Chem.rdchem.BondDir.ENDDOWNRIGHT\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "584a94ad-a1de-4fff-8dec-25216caf5a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_subgraph(Graph, center, percent=0.2):\n",
    "    assert percent <= 1\n",
    "    G = Graph.copy()\n",
    "    num = int(np.floor(len(G.nodes)*percent))\n",
    "    removed = []\n",
    "    temp = [center]\n",
    "    \n",
    "    while len(removed) < num:\n",
    "        neighbors = []\n",
    "        if len(temp) < 1:\n",
    "            break\n",
    "\n",
    "        for n in temp:\n",
    "            neighbors.extend([i for i in G.neighbors(n) if i not in temp])      \n",
    "        for n in temp:\n",
    "            if len(removed) < num:\n",
    "                G.remove_node(n)\n",
    "                removed.append(n)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        temp = list(set(neighbors))\n",
    "    return G, removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83d75212-e227-49b4-a737-41f924eacf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphData(Dataset):\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        GraphData class inheriting from the Dataset class in PyTorch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            The dataframe containing the SMILES strings.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.valid_indices = self._get_valid_indices()\n",
    "\n",
    "    def _get_valid_indices(self):\n",
    "        valid_indices = []\n",
    "        for idx in range(len(self.df)):\n",
    "            row = self.df.iloc[idx]\n",
    "            smiles = row['smiles']\n",
    "            try:\n",
    "                mol = Chem.MolFromSmiles(smiles)\n",
    "                if mol is None:\n",
    "                    raise ValueError(f\"Invalid SMILES: {smiles}\")\n",
    "                valid_indices.append(idx)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing SMILES {smiles}: {e}\")\n",
    "                continue  # Skip invalid SMILES\n",
    "        return valid_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the graph representation of the molecule.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Index of the valid sample.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        graph : torch_geometric.data.Data\n",
    "            The graph representation of the molecule.\n",
    "        \"\"\"\n",
    "        # Use valid index to get the actual row from the dataframe\n",
    "        valid_idx = self.valid_indices[idx]\n",
    "        row = self.df.iloc[valid_idx]\n",
    "        smiles = row['smiles']\n",
    "        \n",
    "        # Process the valid SMILES string\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        mol = Chem.AddHs(mol)\n",
    "        \n",
    "        N = mol.GetNumAtoms()\n",
    "        M = mol.GetNumBonds()\n",
    "        atoms = mol.GetAtoms()\n",
    "        bonds = mol.GetBonds()\n",
    "\n",
    "        #########################\n",
    "        # Get the molecule info #\n",
    "        #########################\n",
    "        type_idx = []\n",
    "        chirality_idx = []\n",
    "        atomic_number = []\n",
    "        for atom in mol.GetAtoms():\n",
    "            type_idx.append(ATOM_LIST.index(atom.GetAtomicNum()))\n",
    "            chirality_idx.append(CHIRALITY_LIST.index(atom.GetChiralTag()))\n",
    "            atomic_number.append(atom.GetAtomicNum())\n",
    "\n",
    "        x1 = torch.tensor(type_idx, dtype=torch.long).view(-1,1)\n",
    "        x2 = torch.tensor(chirality_idx, dtype=torch.long).view(-1,1)\n",
    "        x = torch.cat([x1, x2], dim=-1)\n",
    "\n",
    "        row, col, edge_feat = [], [], []\n",
    "        for bond in mol.GetBonds():\n",
    "            start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "            row += [start, end]\n",
    "            col += [end, start]\n",
    "            edge_feat.append([\n",
    "                BOND_LIST.index(bond.GetBondType()),\n",
    "                BONDDIR_LIST.index(bond.GetBondDir())\n",
    "            ])\n",
    "            edge_feat.append([\n",
    "                BOND_LIST.index(bond.GetBondType()),\n",
    "                BONDDIR_LIST.index(bond.GetBondDir())\n",
    "            ])\n",
    "\n",
    "        edge_index = torch.tensor([row, col], dtype=torch.long)\n",
    "        edge_attr = torch.tensor(np.array(edge_feat), dtype=torch.long)\n",
    "\n",
    "        ####################\n",
    "        # Subgraph Masking #\n",
    "        ####################\n",
    "\n",
    "        # Construct the original molecular graph from edges (bonds)\n",
    "        edges = []\n",
    "        for bond in bonds:\n",
    "            edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])\n",
    "        molGraph = nx.Graph(edges)\n",
    "        \n",
    "        # Get the graph for i and j after removing subgraphs\n",
    "        start_i, start_j = random.sample(list(range(N)), 2)\n",
    "        percent_i, percent_j = random.uniform(0, 0.2), random.uniform(0, 0.2)\n",
    "        G_i, removed_i = remove_subgraph(molGraph, start_i, percent=percent_i)\n",
    "        G_j, removed_j = remove_subgraph(molGraph, start_j, percent=percent_j)\n",
    "\n",
    "        atom_remain_indices_i = [i for i in range(N) if i not in removed_i]\n",
    "        atom_remain_indices_j = [i for i in range(N) if i not in removed_j]\n",
    "        \n",
    "        # Only consider bond still exist after removing subgraph\n",
    "        row_i, col_i, row_j, col_j = [], [], [], []\n",
    "        edge_feat_i, edge_feat_j = [], []\n",
    "        G_i_edges = list(G_i.edges)\n",
    "        G_j_edges = list(G_j.edges)\n",
    "\n",
    "        for bond in mol.GetBonds():\n",
    "            start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "            feature = [\n",
    "                BOND_LIST.index(bond.GetBondType()),\n",
    "                BONDDIR_LIST.index(bond.GetBondDir())\n",
    "            ]\n",
    "            if (start, end) in G_i_edges or (end, start) in G_i_edges:\n",
    "                row_i += [start, end]\n",
    "                col_i += [end, start]\n",
    "                edge_feat_i.append(feature)\n",
    "                edge_feat_i.append(feature)\n",
    "            if (start, end) in G_j_edges or (end, start) in G_j_edges:\n",
    "                row_j += [start, end]\n",
    "                col_j += [end, start]\n",
    "                edge_feat_j.append(feature)\n",
    "                edge_feat_j.append(feature)\n",
    "        \n",
    "        edge_index_i = torch.tensor([row_i, col_i], dtype=torch.long)\n",
    "        edge_attr_i = torch.tensor(np.array(edge_feat_i), dtype=torch.long)\n",
    "        edge_index_j = torch.tensor([row_j, col_j], dtype=torch.long)\n",
    "        edge_attr_j = torch.tensor(np.array(edge_feat_j), dtype=torch.long)\n",
    "\n",
    "        ############################\n",
    "        # Random Atom/Edge Masking #\n",
    "        ############################\n",
    "\n",
    "        num_mask_nodes_i = max([0, math.floor(0.25*N)-len(removed_i)])\n",
    "        num_mask_edges_i = max([0, edge_attr_i.size(0)//2 - math.ceil(0.75*M)])\n",
    "        num_mask_nodes_j = max([0, math.floor(0.25*N)-len(removed_j)])\n",
    "        num_mask_edges_j = max([0, edge_attr_j.size(0)//2 - math.ceil(0.75*M)])\n",
    "        mask_nodes_i = random.sample(atom_remain_indices_i, num_mask_nodes_i)\n",
    "        mask_nodes_j = random.sample(atom_remain_indices_j, num_mask_nodes_j)\n",
    "        mask_edges_i_single = random.sample(list(range(edge_attr_i.size(0)//2)), num_mask_edges_i)\n",
    "        mask_edges_j_single = random.sample(list(range(edge_attr_j.size(0)//2)), num_mask_edges_j)\n",
    "        mask_edges_i = [2*i for i in mask_edges_i_single] + [2*i+1 for i in mask_edges_i_single]\n",
    "        mask_edges_j = [2*i for i in mask_edges_j_single] + [2*i+1 for i in mask_edges_j_single]\n",
    "\n",
    "        x_i = deepcopy(x)\n",
    "        for atom_idx in range(N):\n",
    "            if (atom_idx in mask_nodes_i) or (atom_idx in removed_i):\n",
    "                x_i[atom_idx,:] = torch.tensor([len(ATOM_LIST), 0])\n",
    "        edge_index_final_i = torch.zeros((2, edge_attr_i.size(0) - 2*num_mask_edges_i), dtype=torch.long)\n",
    "        edge_attr_final_i = torch.zeros((edge_attr_i.size(0) - 2*num_mask_edges_i, 2), dtype=torch.long)\n",
    "        count = 0\n",
    "        for bond_idx in range(edge_attr_i.size(0)):\n",
    "            if bond_idx not in mask_edges_i:\n",
    "                edge_index_final_i[:,count] = edge_index_i[:,bond_idx]\n",
    "                edge_attr_final_i[count,:] = edge_attr_i[bond_idx,:]\n",
    "                count += 1\n",
    "        data_i = Data(x=x_i, edge_index=edge_index_final_i, edge_attr=edge_attr_final_i)\n",
    "\n",
    "        x_j = deepcopy(x)\n",
    "        for atom_idx in range(N):\n",
    "            if (atom_idx in mask_nodes_j) or (atom_idx in removed_j):\n",
    "                x_j[atom_idx,:] = torch.tensor([len(ATOM_LIST), 0])\n",
    "        edge_index_final_j = torch.zeros((2, edge_attr_j.size(0) - 2*num_mask_edges_j), dtype=torch.long)\n",
    "        edge_attr_final_j = torch.zeros((edge_attr_j.size(0) - 2*num_mask_edges_j, 2), dtype=torch.long)\n",
    "        count = 0\n",
    "        for bond_idx in range(edge_attr_j.size(0)):\n",
    "            if bond_idx not in mask_edges_j:\n",
    "                edge_index_final_j[:,count] = edge_index_j[:,bond_idx]\n",
    "                edge_attr_final_j[count,:] = edge_attr_j[bond_idx,:]\n",
    "                count += 1\n",
    "        data_j = Data(x=x_j, edge_index=edge_index_final_j, edge_attr=edge_attr_final_j)\n",
    "        \n",
    "        return data_i, data_j\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f246be93-2515-409f-b134-e49585ce45b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphData_unaug(Dataset):\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        GraphData class inheriting from the Dataset class in PyTorch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            The dataframe containing the SMILES strings.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.valid_indices = self._get_valid_indices()\n",
    "\n",
    "    def _get_valid_indices(self):\n",
    "        valid_indices = []\n",
    "        for idx in range(len(self.df)):\n",
    "            row = self.df.iloc[idx]\n",
    "            smiles = row['smiles']\n",
    "            try:\n",
    "                mol = Chem.MolFromSmiles(smiles)\n",
    "                if mol is None:\n",
    "                    raise ValueError(f\"Invalid SMILES: {smiles}\")\n",
    "                valid_indices.append(idx)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing SMILES {smiles}: {e}\")\n",
    "                continue  # Skip invalid SMILES\n",
    "        return valid_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the graph representation of the molecule.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Index of the valid sample.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        graph : torch_geometric.data.Data\n",
    "            The graph representation of the molecule.\n",
    "        \"\"\"\n",
    "        # Use valid index to get the actual row from the dataframe\n",
    "        valid_idx = self.valid_indices[idx]\n",
    "        row = self.df.iloc[valid_idx]\n",
    "        smiles = row['smiles']\n",
    "        \n",
    "        # Process the valid SMILES string\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        mol = Chem.AddHs(mol)\n",
    "        \n",
    "        N = mol.GetNumAtoms()\n",
    "        M = mol.GetNumBonds()\n",
    "\n",
    "        type_idx = []\n",
    "        chirality_idx = []\n",
    "        atomic_number = []\n",
    "        # aromatic = []\n",
    "        # sp, sp2, sp3, sp3d = [], [], [], []\n",
    "        # num_hs = []\n",
    "        for atom in mol.GetAtoms():\n",
    "            type_idx.append(ATOM_LIST.index(atom.GetAtomicNum()))\n",
    "            chirality_idx.append(CHIRALITY_LIST.index(atom.GetChiralTag()))\n",
    "            atomic_number.append(atom.GetAtomicNum())\n",
    "            # aromatic.append(1 if atom.GetIsAromatic() else 0)\n",
    "            # hybridization = atom.GetHybridization()\n",
    "            # sp.append(1 if hybridization == HybridizationType.SP else 0)\n",
    "            # sp2.append(1 if hybridization == HybridizationType.SP2 else 0)\n",
    "            # sp3.append(1 if hybridization == HybridizationType.SP3 else 0)\n",
    "            # sp3d.append(1 if hybridization == HybridizationType.SP3D else 0)\n",
    "\n",
    "        # z = torch.tensor(atomic_number, dtype=torch.long)\n",
    "        x1 = torch.tensor(type_idx, dtype=torch.long).view(-1,1)\n",
    "        x2 = torch.tensor(chirality_idx, dtype=torch.long).view(-1,1)\n",
    "        x = torch.cat([x1, x2], dim=-1)\n",
    "        # x2 = torch.tensor([atomic_number, aromatic, sp, sp2, sp3, sp3d, num_hs],\n",
    "        #                     dtype=torch.float).t().contiguous()\n",
    "        # x = torch.cat([x1.to(torch.float), x2], dim=-1)\n",
    "\n",
    "        row, col, edge_feat = [], [], []\n",
    "        for bond in mol.GetBonds():\n",
    "            start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "            row += [start, end]\n",
    "            col += [end, start]\n",
    "            # edge_type += 2 * [MOL_BONDS[bond.GetBondType()]]\n",
    "            edge_feat.append([\n",
    "                BOND_LIST.index(bond.GetBondType()),\n",
    "                BONDDIR_LIST.index(bond.GetBondDir())\n",
    "            ])\n",
    "            edge_feat.append([\n",
    "                BOND_LIST.index(bond.GetBondType()),\n",
    "                BONDDIR_LIST.index(bond.GetBondDir())\n",
    "            ])\n",
    "\n",
    "        edge_index = torch.tensor([row, col], dtype=torch.long)\n",
    "        edge_attr = torch.tensor(np.array(edge_feat), dtype=torch.long)\n",
    "\n",
    "        # random mask a subgraph of the molecule\n",
    "        num_mask_nodes = max([1, math.floor(0.25*N)])\n",
    "        num_mask_edges = max([0, math.floor(0.25*M)])\n",
    "        mask_nodes_i = random.sample(list(range(N)), num_mask_nodes)\n",
    "        mask_nodes_j = random.sample(list(range(N)), num_mask_nodes)\n",
    "        mask_edges_i_single = random.sample(list(range(M)), num_mask_edges)\n",
    "        mask_edges_j_single = random.sample(list(range(M)), num_mask_edges)\n",
    "        mask_edges_i = [2*i for i in mask_edges_i_single] + [2*i+1 for i in mask_edges_i_single]\n",
    "        mask_edges_j = [2*i for i in mask_edges_j_single] + [2*i+1 for i in mask_edges_j_single]\n",
    "\n",
    "        x_i = deepcopy(x)\n",
    "        for atom_idx in mask_nodes_i:\n",
    "            x_i[atom_idx,:] = torch.tensor([len(ATOM_LIST), 0])\n",
    "        edge_index_i = torch.zeros((2, 2*(M-num_mask_edges)), dtype=torch.long)\n",
    "        edge_attr_i = torch.zeros((2*(M-num_mask_edges), 2), dtype=torch.long)\n",
    "        count = 0\n",
    "        for bond_idx in range(2*M):\n",
    "            if bond_idx not in mask_edges_i:\n",
    "                edge_index_i[:,count] = edge_index[:,bond_idx]\n",
    "                edge_attr_i[count,:] = edge_attr[bond_idx,:]\n",
    "                count += 1\n",
    "        data_i = Data(x=x_i, edge_index=edge_index_i, edge_attr=edge_attr_i)\n",
    "\n",
    "        x_j = deepcopy(x)\n",
    "        for atom_idx in mask_nodes_j:\n",
    "            x_j[atom_idx,:] = torch.tensor([len(ATOM_LIST), 0])\n",
    "        edge_index_j = torch.zeros((2, 2*(M-num_mask_edges)), dtype=torch.long)\n",
    "        edge_attr_j = torch.zeros((2*(M-num_mask_edges), 2), dtype=torch.long)\n",
    "        count = 0\n",
    "        for bond_idx in range(2*M):\n",
    "            if bond_idx not in mask_edges_j:\n",
    "                edge_index_j[:,count] = edge_index[:,bond_idx]\n",
    "                edge_attr_j[count,:] = edge_attr[bond_idx,:]\n",
    "                count += 1\n",
    "        data_j = Data(x=x_j, edge_index=edge_index_j, edge_attr=edge_attr_j)\n",
    "        \n",
    "        return data_i, data_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c97a4d80-d346-4430-b3fe-48bda60023c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6cc42a7-f485-4625-b88d-e7bdd1e6a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops\n",
    "from torch_geometric.nn import global_add_pool, global_mean_pool, global_max_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8895b9ce-9c7c-4fcf-a1a7-8a976e2552ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTXentLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, batch_size, temperature, use_cosine_similarity):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        self.mask_samples_from_same_repr = self._get_correlated_mask().type(torch.bool)\n",
    "        self.similarity_function = self._get_similarity_function(use_cosine_similarity)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "    def _get_similarity_function(self, use_cosine_similarity):\n",
    "        if use_cosine_similarity:\n",
    "            self._cosine_similarity = torch.nn.CosineSimilarity(dim=-1)\n",
    "            return self._cosine_simililarity\n",
    "        else:\n",
    "            return self._dot_simililarity\n",
    "\n",
    "    def _get_correlated_mask(self):\n",
    "        diag = np.eye(2 * self.batch_size)\n",
    "        l1 = np.eye((2 * self.batch_size), 2 * self.batch_size, k=-self.batch_size)\n",
    "        l2 = np.eye((2 * self.batch_size), 2 * self.batch_size, k=self.batch_size)\n",
    "        mask = torch.from_numpy((diag + l1 + l2))\n",
    "        mask = (1 - mask).type(torch.bool)\n",
    "        return mask\n",
    "\n",
    "    @staticmethod\n",
    "    def _dot_simililarity(x, y):\n",
    "        v = torch.tensordot(x.unsqueeze(1), y.T.unsqueeze(0), dims=2)\n",
    "        # x shape: (N, 1, C)\n",
    "        # y shape: (1, C, 2N)\n",
    "        # v shape: (N, 2N)\n",
    "        return v\n",
    "\n",
    "    def _cosine_simililarity(self, x, y):\n",
    "        # x shape: (N, 1, C)\n",
    "        # y shape: (1, 2N, C)\n",
    "        # v shape: (N, 2N)\n",
    "        v = self._cosine_similarity(x.unsqueeze(1), y.unsqueeze(0))\n",
    "        return v\n",
    "\n",
    "    def forward(self, zis, zjs):\n",
    "        representations = torch.cat([zjs, zis], dim=0)\n",
    "\n",
    "        similarity_matrix = self.similarity_function(representations, representations)\n",
    "\n",
    "        # filter out the scores from the positive samples\n",
    "        l_pos = torch.diag(similarity_matrix, self.batch_size)\n",
    "        r_pos = torch.diag(similarity_matrix, -self.batch_size)\n",
    "        positives = torch.cat([l_pos, r_pos]).view(2 * self.batch_size, 1)\n",
    "\n",
    "        negatives = similarity_matrix[self.mask_samples_from_same_repr].view(2 * self.batch_size, -1)\n",
    "\n",
    "        logits = torch.cat((positives, negatives), dim=1)\n",
    "        logits /= self.temperature\n",
    "\n",
    "        labels = torch.zeros(2 * self.batch_size).long()\n",
    "        loss = self.criterion(logits, labels)\n",
    "\n",
    "        return loss / (2 * self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8792b4d-e245-4cd5-b394-811b15c66cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = NTXentLoss(batch_size = 512, temperature = 0.1, use_cosine_similarity=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f66765a-514a-473d-be36-9e9e2c16d613",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_atom_type = 119 # including the extra mask tokens\n",
    "num_chirality_tag = 3\n",
    "\n",
    "num_bond_type = 5 # including aromatic and self-loop edge\n",
    "num_bond_direction = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0e3ea02-c722-4458-bb43-811decde4d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINEConv(MessagePassing):\n",
    "    def __init__(self, emb_dim):\n",
    "        super(GINEConv, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 2*emb_dim), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(2*emb_dim, emb_dim)\n",
    "        )\n",
    "        self.edge_embedding1 = nn.Embedding(num_bond_type, emb_dim)\n",
    "        self.edge_embedding2 = nn.Embedding(num_bond_direction, emb_dim)\n",
    "        nn.init.xavier_uniform_(self.edge_embedding1.weight.data)\n",
    "        nn.init.xavier_uniform_(self.edge_embedding2.weight.data)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # add self loops in the edge space\n",
    "        edge_index = add_self_loops(edge_index, num_nodes=x.size(0))[0]\n",
    "\n",
    "        # add features corresponding to self-loop edges.\n",
    "        self_loop_attr = torch.zeros(x.size(0), 2)\n",
    "        self_loop_attr[:,0] = 4 #bond type for self-loop edge\n",
    "        self_loop_attr = self_loop_attr.to(edge_attr.device).to(edge_attr.dtype)\n",
    "        edge_attr = torch.cat((edge_attr, self_loop_attr), dim=0)\n",
    "\n",
    "        edge_embeddings = self.edge_embedding1(edge_attr[:,0]) + self.edge_embedding2(edge_attr[:,1])\n",
    "\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_embeddings)\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        return x_j + edge_attr\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return self.mlp(aggr_out)\n",
    "\n",
    "\n",
    "class GINet(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_layer (int): the number of GNN layers\n",
    "        emb_dim (int): dimensionality of embeddings\n",
    "        max_pool_layer (int): the layer from which we use max pool rather than add pool for neighbor aggregation\n",
    "        drop_ratio (float): dropout rate\n",
    "        gnn_type: gin, gcn, graphsage, gat\n",
    "    Output:\n",
    "        node representations\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layer=5, emb_dim=300, feat_dim=512, drop_ratio=0, pool='mean'):\n",
    "        super(GINet, self).__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.emb_dim = emb_dim\n",
    "        self.feat_dim = feat_dim\n",
    "        self.drop_ratio = drop_ratio\n",
    "\n",
    "        self.x_embedding1 = nn.Embedding(num_atom_type, emb_dim)\n",
    "        self.x_embedding2 = nn.Embedding(num_chirality_tag, emb_dim)\n",
    "        nn.init.xavier_uniform_(self.x_embedding1.weight.data)\n",
    "        nn.init.xavier_uniform_(self.x_embedding2.weight.data)\n",
    "\n",
    "        # List of MLPs\n",
    "        self.gnns = nn.ModuleList()\n",
    "        for layer in range(num_layer):\n",
    "            self.gnns.append(GINEConv(emb_dim))\n",
    "\n",
    "        # List of batchnorms\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        for layer in range(num_layer):\n",
    "            self.batch_norms.append(nn.BatchNorm1d(emb_dim))\n",
    "        \n",
    "        if pool == 'mean':\n",
    "            self.pool = global_mean_pool\n",
    "        elif pool == 'max':\n",
    "            self.pool = global_max_pool\n",
    "        elif pool == 'add':\n",
    "            self.pool = global_add_pool\n",
    "        \n",
    "        self.feat_lin = nn.Linear(self.emb_dim, self.feat_dim)\n",
    "\n",
    "        self.out_lin = nn.Sequential(\n",
    "            nn.Linear(self.feat_dim, self.feat_dim), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.feat_dim, self.feat_dim//2)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        edge_attr = data.edge_attr\n",
    "\n",
    "        h = self.x_embedding1(x[:,0]) + self.x_embedding2(x[:,1])\n",
    "\n",
    "        for layer in range(self.num_layer):\n",
    "            h = self.gnns[layer](h, edge_index, edge_attr)\n",
    "            h = self.batch_norms[layer](h)\n",
    "            if layer == self.num_layer - 1:\n",
    "                h = F.dropout(h, self.drop_ratio, training=self.training)\n",
    "            else:\n",
    "                h = F.dropout(F.relu(h), self.drop_ratio, training=self.training)\n",
    "\n",
    "        h = self.pool(h, data.batch)\n",
    "        h = self.feat_lin(h)\n",
    "        out = self.out_lin(h)\n",
    "        \n",
    "        return h, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4fafad1-59db-483a-bc92-6594053662b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(external_dir/'toxcast_cleaned.csv', index_col = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43609fa3-e63c-4cb2-85f6-163e45dc349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d233e4f3-e24d-4ed5-adc4-92e9e113c91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df1, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "232bab30-f3ce-4363-b4e3-a436b27a47f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10:28:20] Explicit valence for atom # 5 N, 5, is greater than permitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing SMILES OCC=C1C[NH]2(CC=C)CCC34C2CC1C1=CN2C5C(=CN(C31)C1=C4C=CC=C1)C1CC3C5(CC[NH]3(CC=C)CC1=CCO)C1=C2C=CC=C1: Invalid SMILES: OCC=C1C[NH]2(CC=C)CCC34C2CC1C1=CN2C5C(=CN(C31)C1=C4C=CC=C1)C1CC3C5(CC[NH]3(CC=C)CC1=CCO)C1=C2C=CC=C1\n"
     ]
    }
   ],
   "source": [
    "train_data = GraphData(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "280349cc-84f0-4cd3-80f8-d7c45aeda15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grace/anaconda3/envs/pytorch_cuda/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=512, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d7eec4b-8922-4fbd-b1a0-8ae74977842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = GraphData(test)\n",
    "test_loader = DataLoader(test_data, batch_size=512, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f38aa24a-7cd5-4581-b623-816472bcd82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rax = (pd.read_csv(interim_dir/'icf_processed_220125.csv')\n",
    "       .rename(columns = {'QSAR Ready SMILES':'smiles'})\n",
    " .query('smiles.notnull()')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "665325b7-330d-4f58-b7cd-75a98fd8e53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 755 entries, 0 to 796\n",
      "Data columns (total 12 columns):\n",
      " #   Column                        Non-Null Count  Dtype \n",
      "---  ------                        --------------  ----- \n",
      " 0   Unnamed: 0                    755 non-null    int64 \n",
      " 1   Substance Name in Assessment  755 non-null    object\n",
      " 2   substance_role                755 non-null    object\n",
      " 3   Approach                      755 non-null    object\n",
      " 4   dtxsid                        755 non-null    object\n",
      " 5   Preferred Name                750 non-null    object\n",
      " 6   CASRN                         753 non-null    object\n",
      " 7   SMILES                        755 non-null    object\n",
      " 8   smiles                        755 non-null    object\n",
      " 9   analogue_evidence_stream      678 non-null    object\n",
      " 10  use_case                      755 non-null    object\n",
      " 11  source                        697 non-null    object\n",
      "dtypes: int64(1), object(11)\n",
      "memory usage: 76.7+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grace/anaconda3/envs/pytorch_cuda/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "rax.info()\n",
    "\n",
    "rax_data = GraphData_unaug(rax)\n",
    "rax_loader = DataLoader(rax_data, batch_size=1, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49752b52-9242-435e-a8cc-d4e9248f01b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GINet(feat_dim=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7c4fa3a-b9e7-4751-b1c2-5d109b41ca42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Dimension: 512\n"
     ]
    }
   ],
   "source": [
    "print(f\"Feature Dimension: {model.feat_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a579309a-c7e2-43af-a9b6-236806a5b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr= 0.0005, weight_decay=1e-5   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "875fd92a-aa80-4af5-af01-2eb76f92ce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import SequentialLR, CosineAnnealingLR, LambdaLR\n",
    "\n",
    "# Warm-up for the first 10 epochs\n",
    "warmup_scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: epoch / 10.0 if epoch < 10 else 1.0)\n",
    "\n",
    "# Cosine decay for the remaining epochs\n",
    "cosine_scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=0)\n",
    "\n",
    "# Combine the warm-up and cosine schedulers\n",
    "scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5096f16-d683-4056-b331-7a26a742dccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        for bn, (xis, xjs) in enumerate(val_loader):\n",
    "            # Forward pass\n",
    "            ris, zis = model(xis)\n",
    "            rjs, zjs = model(xjs)\n",
    "\n",
    "            # Normalize projections\n",
    "            zis = F.normalize(zis, dim=1)\n",
    "            zjs = F.normalize(zjs, dim=1)\n",
    "            # Compute loss\n",
    "            loss = criterion(zis, zjs)\n",
    "            total_loss += loss.item()\n",
    "        return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321ae7b7-d52d-4ce4-b25c-9bf13982891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, min_delta=0.0, path='best_model.pt', verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How many epochs to wait after last improvement.\n",
    "            min_delta (float): Minimum change in monitored metric to qualify as an improvement.\n",
    "            path (str): Path to save the best model.\n",
    "            verbose (bool): Whether to print updates about early stopping.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.path = path\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "            if self.verbose:\n",
    "                print(f\"Validation loss improved to {val_loss:.4f}. Model saved!\")\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"No improvement in validation loss for {self.counter}/{self.patience} epochs.\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        \"\"\"Saves the current best model.\"\"\"\n",
    "        torch.save(model.state_dict(), self.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a5b52f-3d27-42cb-8d8f-024b82e5fc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize early stopping\n",
    "early_stopping = EarlyStopping(patience=10, path=\"best_model.pt\", verbose=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training step\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for (xis, xjs) in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        xis, xjs = xis.to(device), xjs.to(device)\n",
    "        loss = model._step(xis, xjs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for (xis, xjs) in valid_loader:\n",
    "            xis, xjs = xis.to(device), xjs.to(device)\n",
    "            loss = model._step(xis, xjs)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(valid_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Validation Loss = {val_loss:.4f}\")\n",
    "\n",
    "    # Check early stopping\n",
    "    early_stopping(val_loss, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered. Training terminated.\")\n",
    "        break\n",
    "\n",
    "# Load the best model after early stopping\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "print(\"Loaded the best model based on validation loss.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "214ad080-2fba-4f89-bfd8-e00f04a20e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 6.0916, LR: 0.000050, Val Loss: 6.7507\n",
      "Epoch [2/50], Loss: 4.7915, LR: 0.000100, Val Loss: 5.7389\n",
      "Epoch [3/50], Loss: 3.9567, LR: 0.000150, Val Loss: 4.3158\n",
      "Epoch [4/50], Loss: 3.4778, LR: 0.000200, Val Loss: 4.0385\n",
      "Epoch [5/50], Loss: 3.3598, LR: 0.000250, Val Loss: 3.7401\n",
      "Epoch [6/50], Loss: 3.0365, LR: 0.000300, Val Loss: 3.8215\n",
      "Epoch [7/50], Loss: 2.8599, LR: 0.000350, Val Loss: 3.7542\n",
      "Epoch [8/50], Loss: 2.8086, LR: 0.000400, Val Loss: 3.2139\n",
      "Epoch [9/50], Loss: 2.6557, LR: 0.000450, Val Loss: 3.4889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grace/anaconda3/envs/pytorch_cuda/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 2.6599, LR: 0.000500, Val Loss: 3.0659\n",
      "Epoch [11/50], Loss: 2.6467, LR: 0.000500, Val Loss: 2.9623\n",
      "Epoch [12/50], Loss: 2.4279, LR: 0.000498, Val Loss: 3.1351\n",
      "Epoch [13/50], Loss: 2.4089, LR: 0.000496, Val Loss: 2.6694\n",
      "Epoch [14/50], Loss: 2.3276, LR: 0.000492, Val Loss: 2.6286\n",
      "Epoch [15/50], Loss: 2.3061, LR: 0.000488, Val Loss: 2.5881\n",
      "Epoch [16/50], Loss: 2.2423, LR: 0.000482, Val Loss: 2.5448\n",
      "Epoch [17/50], Loss: 2.1230, LR: 0.000476, Val Loss: 2.5555\n",
      "Epoch [18/50], Loss: 2.1612, LR: 0.000469, Val Loss: 2.3313\n",
      "Epoch [19/50], Loss: 2.1016, LR: 0.000461, Val Loss: 2.3231\n",
      "Epoch [20/50], Loss: 2.1994, LR: 0.000452, Val Loss: 2.3563\n",
      "Epoch [21/50], Loss: 2.1208, LR: 0.000443, Val Loss: 2.3305\n",
      "Epoch [22/50], Loss: 2.1769, LR: 0.000432, Val Loss: 2.1813\n",
      "Epoch [23/50], Loss: 2.0791, LR: 0.000421, Val Loss: 2.2195\n",
      "Epoch [24/50], Loss: 1.9244, LR: 0.000409, Val Loss: 2.2032\n",
      "Epoch [25/50], Loss: 1.8743, LR: 0.000397, Val Loss: 2.1274\n",
      "Epoch [26/50], Loss: 1.9570, LR: 0.000384, Val Loss: 2.1834\n",
      "Epoch [27/50], Loss: 1.8996, LR: 0.000370, Val Loss: 2.1495\n",
      "Epoch [28/50], Loss: 1.9694, LR: 0.000356, Val Loss: 2.1786\n",
      "Epoch [29/50], Loss: 1.9780, LR: 0.000342, Val Loss: 2.0895\n",
      "Epoch [30/50], Loss: 1.8691, LR: 0.000327, Val Loss: 2.0899\n",
      "Epoch [31/50], Loss: 1.7736, LR: 0.000312, Val Loss: 2.0859\n",
      "Epoch [32/50], Loss: 1.8693, LR: 0.000297, Val Loss: 2.0362\n",
      "Epoch [33/50], Loss: 1.9464, LR: 0.000281, Val Loss: 2.0250\n",
      "Epoch [34/50], Loss: 1.7496, LR: 0.000266, Val Loss: 2.0334\n",
      "Epoch [35/50], Loss: 1.9305, LR: 0.000250, Val Loss: 1.9978\n",
      "Epoch [36/50], Loss: 1.8778, LR: 0.000234, Val Loss: 1.9953\n",
      "Epoch [37/50], Loss: 1.8710, LR: 0.000219, Val Loss: 1.9899\n",
      "Epoch [38/50], Loss: 1.8617, LR: 0.000203, Val Loss: 1.9915\n",
      "Epoch [39/50], Loss: 1.8602, LR: 0.000188, Val Loss: 1.9732\n",
      "Epoch [40/50], Loss: 2.0332, LR: 0.000173, Val Loss: 1.9270\n",
      "Epoch [41/50], Loss: 1.8022, LR: 0.000158, Val Loss: 1.9495\n",
      "Epoch [42/50], Loss: 1.7903, LR: 0.000144, Val Loss: 1.9464\n",
      "Epoch [43/50], Loss: 1.7849, LR: 0.000130, Val Loss: 1.9193\n",
      "Epoch [44/50], Loss: 1.7499, LR: 0.000116, Val Loss: 1.9110\n",
      "Epoch [45/50], Loss: 1.7931, LR: 0.000103, Val Loss: 1.8694\n",
      "Epoch [46/50], Loss: 1.7556, LR: 0.000091, Val Loss: 1.8439\n",
      "Epoch [47/50], Loss: 1.7681, LR: 0.000079, Val Loss: 1.8107\n",
      "Epoch [48/50], Loss: 1.6214, LR: 0.000068, Val Loss: 1.8628\n",
      "Epoch [49/50], Loss: 1.7099, LR: 0.000057, Val Loss: 1.8138\n",
      "Epoch [50/50], Loss: 1.6865, LR: 0.000048, Val Loss: 1.8043\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "embeddings_list = []\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Ensure model is in training mode\n",
    "\n",
    "    for bn, (xis, xjs) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        ris, zis = model(xis)\n",
    "        rjs, zjs = model(xjs)\n",
    "        \n",
    "        # Normalize projections\n",
    "        zis = F.normalize(zis, dim=1)\n",
    "        zjs = F.normalize(zjs, dim=1)\n",
    "        embeddings_list.append((F.normalize(ris, dim=1), F.normalize(rjs, dim=1))) \n",
    "        # Compute loss\n",
    "        loss = criterion(zis, zjs)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    val_loss = validate_model(model, test_loader)\n",
    "\n",
    "    # Step the learning rate scheduler at the end of the epoch\n",
    "    scheduler.step()\n",
    "\n",
    "    # Logging (optional, for monitoring purposes)\n",
    "    current_lr = scheduler.get_last_lr()[0]  # Get current learning rate\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}, LR: {current_lr:.6f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26a24185-7b9e-409f-9004-982e19ada712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 512])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_list[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82e03326-8b30-4830-82be-277a5276c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights_txcst.pth') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aae16b82-c00f-40b3-a026-15b99f8ee854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(model, test_loader):\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (xis, _) in enumerate(test_loader):\n",
    "            \n",
    "            \n",
    "            # Forward pass\n",
    "            ris, _ = model(xis)  # Extract embeddings\n",
    "            \n",
    "            # Normalize\n",
    "            \n",
    "            ris = F.normalize(ris, dim=1)\n",
    "\n",
    "            \n",
    "            all_embeddings.append(ris.cpu().numpy())  \n",
    "\n",
    "    # Stack all embeddings\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    print(f\"Final embeddings shape: {all_embeddings.shape}\")  \n",
    "    \n",
    "    return all_embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb64afc1-9f21-401a-a726-af9c666b74d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final embeddings shape: (755, 512)\n"
     ]
    }
   ],
   "source": [
    "rax_embeddings = extract_embeddings(model, rax_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "325f6970-fdb6-4b29-b4a3-62bef2f75f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "755"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rax_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "637225fc-85e5-4782-ae68-4374c75d318e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "755"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rax_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b57fe3e6-266c-4522-b426-efea29dc522a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "755\n"
     ]
    }
   ],
   "source": [
    "print(len(rax_loader.dataset))  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d7e2dcc-104e-4d1c-b19d-a2468518a773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphData_unaug(755)\n"
     ]
    }
   ],
   "source": [
    "print(rax_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dcc7a6e6-598a-41ea-8cdf-8b527e6fb69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(rax_loader.batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "60f4d125-40c0-4eec-8655-de264c9f1e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Substance Name in Assessment</th>\n",
       "      <th>substance_role</th>\n",
       "      <th>Approach</th>\n",
       "      <th>dtxsid</th>\n",
       "      <th>Preferred Name</th>\n",
       "      <th>CASRN</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>smiles</th>\n",
       "      <th>analogue_evidence_stream</th>\n",
       "      <th>use_case</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Chlorobenzene</td>\n",
       "      <td>Target</td>\n",
       "      <td>Category</td>\n",
       "      <td>DTXSID4020298</td>\n",
       "      <td>Chlorobenzene</td>\n",
       "      <td>108-90-7</td>\n",
       "      <td>ClC1=CC=CC=C1</td>\n",
       "      <td>ClC1=CC=CC=C1</td>\n",
       "      <td>Structural_ChemMine-tools_MCS-Tanimoto | Physc...</td>\n",
       "      <td>technical_guidance</td>\n",
       "      <td>OECD IATA case study</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Substance Name in Assessment substance_role  Approach  \\\n",
       "0           0                Chlorobenzene         Target  Category   \n",
       "\n",
       "          dtxsid Preferred Name     CASRN         SMILES         smiles  \\\n",
       "0  DTXSID4020298  Chlorobenzene  108-90-7  ClC1=CC=CC=C1  ClC1=CC=CC=C1   \n",
       "\n",
       "                            analogue_evidence_stream            use_case  \\\n",
       "0  Structural_ChemMine-tools_MCS-Tanimoto | Physc...  technical_guidance   \n",
       "\n",
       "                 source  \n",
       "0  OECD IATA case study  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rax.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8df155a6-2388-4e9b-9f34-b379edb28949",
   "metadata": {},
   "outputs": [],
   "source": [
    "rax_embeddings_df = (pd.DataFrame(rax_embeddings, index = rax.dtxsid)\n",
    " .rename(columns =lambda x: f'gcn{x}')\n",
    "\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf8569b2-4ec1-43cb-802b-3224e4635dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rax_embeddings_df.to_csv(interim_dir/'rax_txcst_embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a491ee-e1fc-458e-a4e1-1c550cf9997c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb6c8e1-9afa-4a76-8738-fc4c64f6c709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b79ab86-3319-4408-920b-fbb003722f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cuda",
   "language": "python",
   "name": "pytorch_cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
